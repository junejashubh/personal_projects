{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_path = '../data'\n",
    "filename = 'CompleteDataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/05 18:41:51 WARN Utils: Your hostname, Shubhams-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.29.107 instead (on interface en0)\n",
      "24/05/05 18:41:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/05 18:41:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"YourAppName\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df  = spark.read.format('csv').load(os.path.join(ip_path,filename),inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/05 18:48:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Name, Age, Photo, Nationality, Flag, Overall, Potential, Club, Club Logo, Value, Wage, Special, Acceleration, Aggression, Agility, Balance, Ball control, Composure, Crossing, Curve, Dribbling, Finishing, Free kick accuracy, GK diving, GK handling, GK kicking, GK positioning, GK reflexes, Heading accuracy, Interceptions, Jumping, Long passing, Long shots, Marking, Penalties, Positioning, Reactions, Short passing, Shot power, Sliding tackle, Sprint speed, Stamina, Standing tackle, Strength, Vision, Volleys, CAM, CB, CDM, CF, CM, ID, LAM, LB, LCB, LCM, LDM, LF, LM, LS, LW, LWB, Preferred Positions, RAM, RB, RCB, RCM, RDM, RF, RM, RS, RW, RWB, ST\n",
      " Schema: _c0, Name, Age, Photo, Nationality, Flag, Overall, Potential, Club, Club Logo, Value, Wage, Special, Acceleration, Aggression, Agility, Balance, Ball control, Composure, Crossing, Curve, Dribbling, Finishing, Free kick accuracy, GK diving, GK handling, GK kicking, GK positioning, GK reflexes, Heading accuracy, Interceptions, Jumping, Long passing, Long shots, Marking, Penalties, Positioning, Reactions, Short passing, Shot power, Sliding tackle, Sprint speed, Stamina, Standing tackle, Strength, Vision, Volleys, CAM, CB, CDM, CF, CM, ID, LAM, LB, LCB, LCM, LDM, LF, LM, LS, LW, LWB, Preferred Positions, RAM, RB, RCB, RCM, RDM, RF, RM, RS, RW, RWB, ST\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/shubhamjuneja/vscode/personal_projects/pyspark/data/CompleteDataset.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()\n",
    "df2 = df.repartition(4)\n",
    "df2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: int, Name: string, Age: int, Photo: string, Nationality: string, Flag: string, Overall: int, Potential: int, Club: string, Club Logo: string, Value: string, Wage: string, Special: int, Acceleration: string, Aggression: string, Agility: string, Balance: string, Ball control: string, Composure: string, Crossing: string, Curve: string, Dribbling: string, Finishing: string, Free kick accuracy: string, GK diving: string, GK handling: string, GK kicking: string, GK positioning: string, GK reflexes: string, Heading accuracy: string, Interceptions: string, Jumping: string, Long passing: string, Long shots: string, Marking: string, Penalties: string, Positioning: string, Reactions: string, Short passing: string, Shot power: string, Sliding tackle: string, Sprint speed: string, Stamina: string, Standing tackle: string, Strength: string, Vision: string, Volleys: string, CAM: double, CB: double, CDM: double, CF: double, CM: double, ID: int, LAM: double, LB: double, LCB: double, LCM: double, LDM: double, LF: double, LM: double, LS: double, LW: double, LWB: double, Preferred Positions: string, RAM: double, RB: double, RCB: double, RCM: double, RDM: double, RF: double, RM: double, RS: double, RW: double, RWB: double, ST: double]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns\n",
    "df2.withColumnRenamed('_c0','Id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|    Id|         Name|\n",
      "+------+-------------+\n",
      "|195540|     A. Abbas|\n",
      "|208915|     A. Abbas|\n",
      "|198076| A. Abdennour|\n",
      "|199282|   A. Abrashi|\n",
      "|198740|    A. Acosta|\n",
      "|201223|    A. Acquah|\n",
      "|204845|    A. Affane|\n",
      "|197678|   A. Ahamada|\n",
      "|193029| A. Ajdarević|\n",
      "|203458|     A. Ajeti|\n",
      "|205234|   A. Akaichi|\n",
      "|201214|  A. Akhyadov|\n",
      "|191847|   A. Al Asta|\n",
      "|210924|A. Al Barakah|\n",
      "|191262|A. Al Dawsari|\n",
      "|210902|A. Al Dawsari|\n",
      "|208926|  A. Al Enezi|\n",
      "|210898|  A. Al Fahad|\n",
      "|210183|  A. Al Fahmi|\n",
      "|209223|  A. Al Faraj|\n",
      "+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select('Id','Name').where((df2['Id']>191104) & (df2['Id']<215061)).sort('Name','Id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanded Funntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/05 19:17:14 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df2.createOrReplaceTempView('fooltball_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|Age|count|\n",
      "+---+-----+\n",
      "| 17|    1|\n",
      "| 18|   13|\n",
      "| 19|   41|\n",
      "| 20|  105|\n",
      "| 21|  160|\n",
      "| 22|  246|\n",
      "| 23|  296|\n",
      "| 24|  369|\n",
      "| 25|  486|\n",
      "| 26|  418|\n",
      "| 27|  444|\n",
      "| 28|  395|\n",
      "| 29|  474|\n",
      "| 30|  359|\n",
      "| 31|  272|\n",
      "| 32|  206|\n",
      "| 33|  283|\n",
      "| 34|  101|\n",
      "| 35|   74|\n",
      "| 36|   57|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_query = \"\"\" Select Age, count(*) as count\n",
    "                from  fooltball_df\n",
    "                where overall>70\n",
    "                group by age \n",
    "                order by age\"\"\"\n",
    "\n",
    "result = spark.sql(sql_query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|        new_name|new_nationality|\n",
      "+----------------+---------------+\n",
      "|DANI ROCHELINHAS|         Brazil|\n",
      "|         D. Léon|         France|\n",
      "|      A. Losilla|         France|\n",
      "|    KIM JONG WOO| KOREA REPUBLIC|\n",
      "|         A. Yoda|         France|\n",
      "|        H. Sacko|         France|\n",
      "|     L. CARTELDO|          Chile|\n",
      "|       J. Bacuna|    NETHERLANDS|\n",
      "|    S. GALLAGHER|        England|\n",
      "|      A. Baredia|          Chile|\n",
      "|        S. Clark|  UNITED STATES|\n",
      "|       M. Araujo|           Peru|\n",
      "|        S. Byram|        England|\n",
      "|       A. Fidler|         Russia|\n",
      "|      H. Çinemre|         Turkey|\n",
      "|  C. OIKONOMIDIS|      Australia|\n",
      "|      S. Ulreich|        Germany|\n",
      "|     JOO MIN KYU| KOREA REPUBLIC|\n",
      "|  THIAGO CARDOSO|         Brazil|\n",
      "|       Guilherme|         Brazil|\n",
      "+----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def convert_uppercase(record):\n",
    "    if len(record)>10:\n",
    "        return record.upper()\n",
    "    else:\n",
    "        return record\n",
    "\n",
    "\n",
    "spark.udf.register('sql_upper',convert_uppercase)\n",
    "\n",
    "sql_query = \"\"\" Select sql_upper(Name) as new_name, sql_upper(Nationality) as new_nationality\n",
    "                from fooltball_df\"\"\"\n",
    "\n",
    "result = spark.sql(sql_query)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
